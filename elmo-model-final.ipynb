{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8028094,"sourceType":"datasetVersion","datasetId":4731583},{"sourceId":8054605,"sourceType":"datasetVersion","datasetId":4750435},{"sourceId":8056080,"sourceType":"datasetVersion","datasetId":4751467},{"sourceId":8057067,"sourceType":"datasetVersion","datasetId":4752195},{"sourceId":8057085,"sourceType":"datasetVersion","datasetId":4752208},{"sourceId":8066629,"sourceType":"datasetVersion","datasetId":4759164},{"sourceId":8066925,"sourceType":"datasetVersion","datasetId":4759390},{"sourceId":8074184,"sourceType":"datasetVersion","datasetId":4764681},{"sourceId":8074415,"sourceType":"datasetVersion","datasetId":4764872},{"sourceId":8074587,"sourceType":"datasetVersion","datasetId":4764989}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ELMO Implementation version 2","metadata":{}},{"cell_type":"code","source":"!pip install indic-nlp-library wandb tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport re\nimport numpy as np\nfrom indicnlp.tokenize import indic_tokenize\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:32:40.169171Z","iopub.execute_input":"2024-04-14T10:32:40.169432Z","iopub.status.idle":"2024-04-14T10:33:01.650886Z","shell.execute_reply.started":"2024-04-14T10:32:40.169408Z","shell.execute_reply":"2024-04-14T10:33:01.649835Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting indic-nlp-library\n  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nCollecting sphinx-argparse (from indic-nlp-library)\n  Downloading sphinx_argparse-0.4.0-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (0.2.4)\nCollecting morfessor (from indic-nlp-library)\n  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (2.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (1.26.4)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.44.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.4)\nCollecting sphinx>=1.2.0 (from sphinx-argparse->indic-nlp-library)\n  Downloading sphinx-7.2.6-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nCollecting sphinxcontrib-applehelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-devhelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-jsmath (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl.metadata (2.4 kB)\nCollecting sphinxcontrib-qthelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\nRequirement already satisfied: Pygments>=2.14 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.17.2)\nRequirement already satisfied: docutils<0.21,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.20.1)\nRequirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\nRequirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\nCollecting alabaster<0.8,>=0.7 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\nCollecting imagesize>=1.3 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.1)\nDownloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\nDownloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\nDownloading sphinx-7.2.6-py3-none-any.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading alabaster-0.7.16-py3-none-any.whl (13 kB)\nDownloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\nDownloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl (120 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl (83 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nDownloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, imagesize, alabaster, sphinx, sphinx-argparse, indic-nlp-library\nSuccessfully installed alabaster-0.7.16 imagesize-1.4.1 indic-nlp-library-0.92 morfessor-2.0.6 sphinx-7.2.6 sphinx-argparse-0.4.0 sphinxcontrib-applehelp-1.0.8 sphinxcontrib-devhelp-1.0.6 sphinxcontrib-htmlhelp-2.0.5 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.7 sphinxcontrib-serializinghtml-1.1.10\n","output_type":"stream"}]},{"cell_type":"code","source":"import fasttext\nimport fasttext.util\nft_model = fasttext.load_model('/kaggle/input/pre-trained-model-indicft/indicnlp.ft.mr.300.bin')\nword = \"नृत्य\"\nprint(\"Embedding Shape is {}\".format(ft_model.get_word_vector(word)))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:33:01.652600Z","iopub.execute_input":"2024-04-14T10:33:01.653006Z","iopub.status.idle":"2024-04-14T10:33:45.154213Z","shell.execute_reply.started":"2024-04-14T10:33:01.652948Z","shell.execute_reply":"2024-04-14T10:33:45.153430Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Embedding Shape is [ 0.41362646 -0.0721162  -0.09042576 -0.0080447   0.24940777 -0.0564224\n -0.06691281 -0.02353338 -0.01068915  0.22869106 -0.05456081  0.05685291\n -0.3771104   0.17173615 -0.19166155  0.05876774  0.2110943  -0.06690847\n -0.17985937  0.19748911 -0.17697716  0.0982177  -0.73754513  0.26441753\n -0.14425668 -0.3502157  -0.0930915  -0.26033682  0.04246099 -0.0807714\n  0.25401157  0.62080336  0.02260456  0.16584569 -0.08181361  0.18448925\n  0.06636861  0.18036523 -0.24447897  0.0946254  -0.05784336  0.27843988\n -0.09996741  0.14146516 -0.2521708  -0.01767177 -0.03513876  0.16193527\n -0.4139789  -0.06065518 -0.13225324  0.0381115   0.404005   -0.39212966\n  0.45432544 -0.18739994  0.16050169 -0.41535494  0.09758026  0.12121335\n -0.464044    0.05734312 -0.11185544  0.0205804  -0.03070647 -0.02953663\n  0.43329865 -0.25726065 -0.2399962   0.17885959  0.03350684  0.03437545\n -0.43484426 -0.05221066  0.07860021 -0.32815468  0.3373454  -0.16823411\n  0.5529572  -0.2737693   0.32174182 -0.09589199 -0.26504675  0.28228852\n -0.13300832 -0.34194663 -0.23633961  0.23643552 -0.1863088  -0.11295956\n -0.12844151  0.02410839  0.23932253  0.3215311   0.05699671  0.22413191\n  0.08045374  0.2986026   0.3022096   0.53993255  0.08275323 -0.07324325\n -0.01760782 -0.13903959 -0.1386919   0.29743928  0.37393937 -0.05383165\n  0.3376111  -0.1218731  -0.04845749 -0.02719295  0.2356913  -0.06848311\n  0.16359642 -0.41534087  0.12944165  0.10274712 -0.0350487  -0.27017096\n -0.00476478 -0.12709253  0.223917    0.26346132  0.64647233  0.3336649\n  0.4591554  -0.12391054  0.40742704  0.12322223 -0.08840033  0.1562795\n -0.17422101 -0.06069856 -0.01211178  0.3650906   0.31428733 -0.13762164\n -0.13915221 -0.36531103  0.31360853 -0.34489873 -0.35958293 -0.18043214\n -0.49166912  0.1105262   0.08837956 -0.07624004 -0.22588837  0.05704039\n  0.12336388 -0.1976308  -0.1602425   0.10362364 -0.17147109 -0.4615524\n  0.2106354   0.10492675 -0.5703714   0.19088392 -0.18732558  0.16088231\n  0.08960537 -0.12914313  0.11163062 -0.0691427  -0.19894136  0.6528754\n -0.14489312 -0.1388297   0.3574635   0.16959412  0.38441172  0.13578413\n  0.3331694  -0.29431045  0.2945102   0.14993976  0.1068694  -0.26339436\n -0.10124314  0.23849963 -0.05221334  0.26607636  0.06981889 -0.13682906\n  0.09398759  0.3270351  -0.08216549 -0.13173361  0.35574827  0.1329823\n -0.24231958 -0.1679359  -0.2933377   0.01353712  0.0697122  -0.01895443\n -0.12301171  0.04092169 -0.5085769   0.07222035  0.05598452  0.15034717\n  0.25121158 -0.15400241 -0.30341262  0.2739817   0.11113145  0.08745019\n  0.22668792  0.35131556 -0.45028406 -0.11277192 -0.12178653 -0.09320896\n  0.4356089   0.14333539  0.01138502  0.17545615 -0.13355182 -0.06609148\n  0.12966566  0.10831398 -0.15840423  0.24485192  0.05393905  0.11686367\n  0.15139994 -0.04787437 -0.2846401   0.24994774  0.15393919  0.31106606\n -0.29380554 -0.17343974  0.01227076 -0.01129472  0.35402337 -0.22968815\n -0.06261022  0.06189056  0.27098823  0.5612617   0.21410607 -0.07809044\n  0.02704465 -0.14772189  0.2430482  -0.17205933  0.27862257  0.37792826\n  0.05182158 -0.25514093 -0.3639677  -0.41423097 -0.25365356  0.04740621\n -0.11953269  0.07387138 -0.43794736  0.27430785 -0.17859057  0.0664492\n  0.17608935  0.01722901 -0.05811734 -0.30715162 -0.11037805 -0.20230514\n -0.23278236 -0.3181451   0.36948442 -0.36806813 -0.21855831  0.0774572\n  0.28538442  0.2131257   0.3051645  -0.15835433  0.0941645   0.01648157\n -0.0320175   0.15249118 -0.25329295  0.16659673  0.18720658 -0.1483968\n  0.13572699 -0.12341308 -0.08829101  0.14806326  0.18834753  0.24253261\n  0.1640515  -0.03044179  0.2775561  -0.33314982 -0.27408773  0.03577875]\n","output_type":"stream"},{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nfrom indicnlp.tokenize import indic_tokenize\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nimport fasttext\nfrom tqdm import tqdm\n\nfolder_path = '/kaggle/input/micro-marathi-dataset'\nmodel_path = '/kaggle/input/pre-trained-model-indicft/indicnlp.ft.mr.300.bin'  # Path to the INDICFT model\n\n# loading the pre-trained model\nft_model = fasttext.load_model(model_path)\n\ntoken_to_index = {'<PAD>': 0, '<UNK>': 1, '<SOS>':2, '<EOS>':3}\nnext_token_index = 4  \n\n# This might be unnecessary if using fixed embeddings from INDICFT, but keeping for consistency\ndef update_indices(token_list, token_to_index):\n    global next_token_index\n    for token in token_list:\n        if token not in token_to_index:\n            token_to_index[token] = next_token_index\n            next_token_index += 1\n\ntexts = []  \nthreshold = 256\n\n# Loading and preprocessing texts\nfor file_name in os.listdir(folder_path):\n    if file_name.endswith('.txt'):\n        file_path = os.path.join(folder_path, file_name)\n        with open(file_path, 'r', encoding='utf-8') as f:\n            text = f.read()\n        sentences = re.split(r'[।\\n\\.]+', text)\n        sentences = [\"<SOS> \"+sentence.strip()+\" <EOS>\" for sentence in sentences if sentence.strip()]\n\n        for sentence in sentences:\n            tokens = indic_tokenize.trivial_tokenize(sentence, lang='mr')\n            update_indices(tokens, token_to_index)\n            if len(tokens) > threshold:  \n                continue  \n            texts.append(sentence)\n\nprint(f\"Number of sentences processed: {len(texts)}\")\n\n# Custom Dataset class for handling Marathi data\nclass MarathiDataset(Dataset):\n    def __init__(self, texts, ft_model, token_to_index):\n        self.texts = texts\n        self.ft_model = ft_model\n        self.token_to_index = token_to_index\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        sentence = self.texts[idx]\n        tokens = indic_tokenize.trivial_tokenize(sentence, lang='mr')\n        embeddings = [self.ft_model.get_word_vector(token) for token in tokens]  # Get embeddings for each token\n        input_embeddings = torch.tensor(embeddings[:-1], dtype=torch.float) # Exclude the last token for input\n        target_indices = [self.token_to_index.get(token, self.token_to_index['<UNK>']) for token in tokens[1:]]  # Exclude the first token for target\n        target_indices = torch.tensor(target_indices, dtype=torch.long)  # Targets are the indices of the next token\n        return input_embeddings, target_indices\n\n# Padding function for batches\ndef collate_fn(batch):\n    (inputs, targets) = zip(*batch)\n    input_embeddings = pad_sequence(inputs, batch_first=True, padding_value=0.0)  # Padding embeddings\n    target_sequences = pad_sequence(targets, batch_first=True, padding_value=token_to_index['<PAD>'])  # Padding target indices\n    return input_embeddings, target_sequences\n\ndataset = MarathiDataset(texts, ft_model, token_to_index)\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n\nfor input_data, targets in dataloader:\n    print(f\"Input batch shape: {input_data.shape}\")\n    print(f\"Target batch shape: {targets.shape}\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:33:45.155726Z","iopub.execute_input":"2024-04-14T10:33:45.156188Z","iopub.status.idle":"2024-04-14T10:33:49.143284Z","shell.execute_reply.started":"2024-04-14T10:33:45.156154Z","shell.execute_reply":"2024-04-14T10:33:49.142379Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"},{"name":"stdout","text":"Number of sentences processed: 1674\nInput batch shape: torch.Size([4, 24, 300])\nTarget batch shape: torch.Size([4, 24])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2193474953.py:62: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  input_embeddings = torch.tensor(embeddings[:-1], dtype=torch.float) # Exclude the last token for input\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ELMoLanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super(ELMoLanguageModel, self).__init__()\n        \n        # Forward and backward LSTMs, extracting all layer outputs\n        self.forward_lstm1 = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)\n        self.forward_lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers=1, batch_first=True)\n        self.backward_lstm1 = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)\n        self.backward_lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers=1, batch_first=True)\n\n        # Linear layers for final predictions\n        self.forward_pred = nn.Linear(hidden_dim, vocab_size)\n        self.backward_pred = nn.Linear(hidden_dim, vocab_size)\n\n        # Weight parameters for combining the layers\n        self.gamma = nn.Parameter(torch.ones(3)) \n\n    def forward(self, x):\n    \n        forward_out1, _ = self.forward_lstm1(x)\n        forward_out2, _ = self.forward_lstm2(forward_out1)\n\n        reversed_embeddings = torch.flip(x, [1])\n        backward_out1, _ = self.backward_lstm1(reversed_embeddings)\n        backward_out2, _ = self.backward_lstm2(backward_out1)\n\n        # Flipping backward outputs back to original sequence order\n        backward_out1 = torch.flip(backward_out1, [1])\n        backward_out2 = torch.flip(backward_out2, [1])\n\n        # last hidden states for predictions\n        forward_predictions = self.forward_pred(forward_out2[:, -1, :])\n        backward_predictions = self.backward_pred(backward_out2[:, 0, :])\n\n        # Weighted sum of embeddings and LSTM outputs\n        combined_embeddings = self.gamma[0] * x + self.gamma[1] * torch.cat((forward_out1, backward_out1), dim=-1) + self.gamma[2] * torch.cat((forward_out2, backward_out2), dim=-1)\n\n        return forward_predictions, backward_predictions, combined_embeddings\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:33:49.146092Z","iopub.execute_input":"2024-04-14T10:33:49.146377Z","iopub.status.idle":"2024-04-14T10:33:49.158122Z","shell.execute_reply.started":"2024-04-14T10:33:49.146352Z","shell.execute_reply":"2024-04-14T10:33:49.157253Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"cuda_available = torch.cuda.is_available()\nprint(\"CUDA Available:\", cuda_available)\ndevice = torch.device(\"cuda\" if cuda_available else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:33:49.159086Z","iopub.execute_input":"2024-04-14T10:33:49.159346Z","iopub.status.idle":"2024-04-14T10:33:49.198849Z","shell.execute_reply.started":"2024-04-14T10:33:49.159319Z","shell.execute_reply":"2024-04-14T10:33:49.197984Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"CUDA Available: True\n","output_type":"stream"}]},{"cell_type":"code","source":"hidden_dim = 150  \nnum_layers = 2  \nvocab_size = len(token_to_index) + 1\n\nmodel = ELMoLanguageModel(vocab_size, 300, hidden_dim).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=token_to_index['<PAD>']) \noptimizer = torch.optim.Adam(model.parameters()) \n\n# Training Loop\nnum_epochs = 3  \nfor epoch in range(num_epochs):\n    model.train() \n    total_loss = 0\n    for input_data, targets in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n        input_data, targets = input_data.to(device), targets.to(device)\n        optimizer.zero_grad()  \n        forward_pred, backward_pred, _ = model(input_data) , _ = self.forward(inputs)\n                \n        loss_f = criterion(forward_pred, targets[:, 1]) \n        loss_b = criterion(backward_pred, targets[:, -1])  \n\n        total_loss = loss_f + loss_b\n        \n        total_loss.backward()  # Backpropagate the loss\n        optimizer.step()       # Updating the weights\n        total_loss += total_loss.item()\n    avg_loss = total_loss / len(dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.15f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:33:49.199847Z","iopub.execute_input":"2024-04-14T10:33:49.200122Z","iopub.status.idle":"2024-04-14T10:34:10.698332Z","shell.execute_reply.started":"2024-04-14T10:33:49.200098Z","shell.execute_reply":"2024-04-14T10:34:10.697280Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 419/419 [00:06<00:00, 62.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Average Loss: 0.000006811817457\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 419/419 [00:06<00:00, 67.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3, Average Loss: 0.000002561656174\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 419/419 [00:06<00:00, 68.45it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3, Average Loss: 0.000001347338753\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model_path = './bilm_marathi_model.pth'\ntorch.save(model.state_dict(), model_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:34:10.699870Z","iopub.execute_input":"2024-04-14T10:34:10.700463Z","iopub.status.idle":"2024-04-14T10:34:10.738839Z","shell.execute_reply.started":"2024-04-14T10:34:10.700421Z","shell.execute_reply":"2024-04-14T10:34:10.737994Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import json\n\nmappings_path = './marathi_mappings.json'\nwith open(mappings_path, 'w', encoding='utf-8') as f:\n    json.dump({\n        'token_to_index': token_to_index\n    }, f, ensure_ascii=False, indent=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:34:10.740364Z","iopub.execute_input":"2024-04-14T10:34:10.740656Z","iopub.status.idle":"2024-04-14T10:34:10.768224Z","shell.execute_reply.started":"2024-04-14T10:34:10.740630Z","shell.execute_reply":"2024-04-14T10:34:10.767383Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## News classification","metadata":{}},{"cell_type":"code","source":"!pip install indic-nlp-library","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:34:10.769366Z","iopub.execute_input":"2024-04-14T10:34:10.769744Z","iopub.status.idle":"2024-04-14T10:34:22.847567Z","shell.execute_reply.started":"2024-04-14T10:34:10.769708Z","shell.execute_reply":"2024-04-14T10:34:22.846401Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: indic-nlp-library in /opt/conda/lib/python3.10/site-packages (0.92)\nRequirement already satisfied: sphinx-argparse in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (0.4.0)\nRequirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (0.2.4)\nRequirement already satisfied: morfessor in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (2.0.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (2.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.4)\nRequirement already satisfied: sphinx>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx-argparse->indic-nlp-library) (7.2.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\nRequirement already satisfied: sphinxcontrib-applehelp in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.8)\nRequirement already satisfied: sphinxcontrib-devhelp in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\nRequirement already satisfied: sphinxcontrib-jsmath in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\nRequirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.5)\nRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.10)\nRequirement already satisfied: sphinxcontrib-qthelp in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\nRequirement already satisfied: Pygments>=2.14 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.17.2)\nRequirement already satisfied: docutils<0.21,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.20.1)\nRequirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\nRequirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\nRequirement already satisfied: alabaster<0.8,>=0.7 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.16)\nRequirement already satisfied: imagesize>=1.3 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\nRequirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\nRequirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nfrom indicnlp.tokenize import indic_tokenize\n\ndef preprocess_text(text, language='mr'):\n    \"\"\"\n    Apply preprocessing steps to the given text.\n    \"\"\"\n    text = remove_non_textual_elements(text)\n    text = normalize_quotation_marks(text)\n    text = ensure_utf8_encoding(text)\n    sentences = tokenize_sentences(text)\n    sentences_SOS = [\"<SOS> \"+sentence+\" <EOS>\" for sentence in sentences]\n    tokenized_sentences = [tokenize_words_indicnlp(sentence, language) for sentence in sentences_SOS]\n    return ' '.join([' '.join(sentence) for sentence in tokenized_sentences])\n\ndef remove_non_textual_elements(text):\n    text = re.sub(r'<[^>]+>', '', text)\n    text = re.sub(r'http\\S+', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef normalize_quotation_marks(text):\n    text = text.replace('“', '\"').replace('”', '\"')\n    text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n    return text\n\ndef ensure_utf8_encoding(text):\n    return text.encode('utf-8', errors='ignore').decode('utf-8')\n\ndef tokenize_sentences(text):\n    sentences = re.split(r'[।\\n\\.]+', text)\n    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n    return sentences\n\ndef tokenize_words_indicnlp(sentence, language='mr'):\n    return indic_tokenize.trivial_tokenize(sentence, lang=language)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:34:22.850696Z","iopub.execute_input":"2024-04-14T10:34:22.851020Z","iopub.status.idle":"2024-04-14T10:34:22.862399Z","shell.execute_reply.started":"2024-04-14T10:34:22.850985Z","shell.execute_reply":"2024-04-14T10:34:22.861455Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!pip install pandas pyarrow","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:34:22.863633Z","iopub.execute_input":"2024-04-14T10:34:22.863946Z","iopub.status.idle":"2024-04-14T10:34:34.811887Z","shell.execute_reply.started":"2024-04-14T10:34:22.863916Z","shell.execute_reply":"2024-04-14T10:34:34.810789Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.4)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (15.0.2)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ndef load_dataset(parquet_path):\n    \"\"\"Load dataset from a Parquet file.\"\"\"\n    return pd.read_parquet(parquet_path)\n\ntrain_path = '/kaggle/input/news-category-classification/marathi/train-00000-of-00001.parquet'\ntest_path = '/kaggle/input/news-category-classification/marathi/test-00000-of-00001.parquet'\nval_path = '/kaggle/input/news-category-classification/marathi/validation-00000-of-00001.parquet'\n\ntrain_df = load_dataset(train_path)\ntest_df = load_dataset(test_path)\nval_df = load_dataset(val_path)\n\nprint(\"Train Dataset:\", train_df.head())\nprint(\"Test Dataset:\", test_df.head())\nprint(\"Validation Dataset:\", val_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:34:34.813369Z","iopub.execute_input":"2024-04-14T10:34:34.813669Z","iopub.status.idle":"2024-04-14T10:34:35.813933Z","shell.execute_reply.started":"2024-04-14T10:34:34.813635Z","shell.execute_reply":"2024-04-14T10:34:35.812945Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Train Dataset:                                                 text  label\n0           …म्हणून सानिया मिर्झाची ड्यू डेट आहे खास      0\n1  Video : दीपिका-रणवीरच्या लग्नाचा मेन्यू झाला लीक!      0\n2   सचिनच्या रणजी कारकीर्दीचा शेवट गोड, मुंबईचा विजय      3\n3  पुरंदरेंना महाराष्ट्र भूषण पुरस्काराविरोधात सं...      4\n4  जयपूर पोलिसांच्या जाहिरातीवर बुमराह संतापला, स...      3\nTest Dataset:                                                 text  label\n0  काजोल पुन्हा माझ्या आयुष्यात येणार नाही - करण ...      0\n1             विराटला चीअर करण्यासाठी अनुष्का सिडनीत      0\n2                                     संतोषचा अड्डा!      0\n3  मुरूड समुद्रात बुडालेल्या विद्यार्थ्याचा मृतदे...      4\n4  गोव्यात शिवसेना वेलिंगकरांसोबत,लवकरच युतीची घोषणा      4\nValidation Dataset:                                                 text  label\n0  CWG 2018 : संजिता चानूची सुवर्णभरारी, भारताच्य...      3\n1  कर्मचार्‍यांच्या हलगर्जीपणामुळे होणार होते जिव...      4\n2                श्रीलंकेचा भारतावर 7 गडी राखून विजय      3\n3               औरंगाबाद महापौरपदाचा तिढा अखेर सुटला      4\n4  औरंगाबाद विद्यापीठाचा भोंगळ कारभार; गोदामात दि...      4\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_dataset(df, text_column='text'):\n    df[text_column] = df[text_column].apply(lambda x: preprocess_text(x))\n    return df\n\ntrain_df_preprocessed = preprocess_dataset(train_df, 'text')\ntest_df_preprocessed = preprocess_dataset(test_df, 'text')\nval_df_preprocessed = preprocess_dataset(val_df, 'text')\n\ntexts_train = train_df_preprocessed['text'].tolist()\ntexts_test = test_df_preprocessed['text'].tolist()\ntexts_val = val_df_preprocessed['text'].tolist()\nlabels_train = train_df_preprocessed['label'].tolist()\nlabels_test = test_df_preprocessed['label'].tolist()\nlabels_val = val_df_preprocessed['label'].tolist()\n\nprint(train_df_preprocessed.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:38:03.113079Z","iopub.execute_input":"2024-04-14T10:38:03.113480Z","iopub.status.idle":"2024-04-14T10:38:03.756233Z","shell.execute_reply.started":"2024-04-14T10:38:03.113448Z","shell.execute_reply":"2024-04-14T10:38:03.755331Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"                                                text  label\n0  < SOS > …म्हणून सानिया मिर्झाची ड्यू डेट आहे ख...      0\n1  < SOS > Video : दीपिका - रणवीरच्या लग्नाचा मेन...      0\n2  < SOS > सचिनच्या रणजी कारकीर्दीचा शेवट गोड , म...      3\n3  < SOS > पुरंदरेंना महाराष्ट्र भूषण पुरस्कारावि...      4\n4  < SOS > जयपूर पोलिसांच्या जाहिरातीवर बुमराह सं...      3\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import classification_report\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport numpy as np\n\n\n\nclass MarathiDatasetCreate(Dataset):\n    def __init__(self, texts, labels, ft_model, token_to_index, lang='mr'):\n        self.texts = texts\n        self.labels = labels\n        self.ft_model = ft_model\n        self.token_to_index = token_to_index\n        self.lang = lang\n\n        # Normalize labels\n        self.label_encoder = LabelEncoder()\n        self.labels = self.label_encoder.fit_transform(labels)\n\n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        tokens = indic_tokenize.trivial_tokenize(text, lang=self.lang)\n        embeddings = [self.ft_model.get_word_vector(token) for token in tokens]\n        input_embeddings = torch.tensor(embeddings, dtype=torch.float)\n        target_label = torch.tensor(label, dtype=torch.long)\n        return input_embeddings, target_label\n\n    def get_label_encoder(self):\n        return self.label_encoder\n\ndef collate_fn(batch):\n    inputs, labels = zip(*batch)\n    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0.0)\n    labels = torch.tensor(labels, dtype=torch.long)\n    return inputs_padded, labels\n\nclass BiLSTMClassifier(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=1, dropout=0.5):\n        super(BiLSTMClassifier, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers,\n                            batch_first=True, bidirectional=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # Multiply by 2 for bidirectional output\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        last_outputs = lstm_out[:, -1, :]  # Get the last time step output\n        output = self.fc(last_outputs)\n        return output\n    \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvocab_size = len(token_to_index)  \nhidden_dim = 150\ninput_dim = 300  \nnum_classes = len(set(train_df_preprocessed['label']))\n\nelmo_model = ELMoLanguageModel(vocab_size, 300, hidden_dim).to(device)\nclassifier = BiLSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(elmo_model.parameters()) + list(classifier.parameters()), lr=0.001)\n\ntrain_dataset = MarathiDatasetCreate(texts_train, labels_train, ft_model, token_to_index)\nval_dataset = MarathiDatasetCreate(texts_val, labels_val, ft_model, token_to_index)\ntest_dataset = MarathiDatasetCreate(texts_test, labels_test, ft_model, token_to_index)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n\n# Training Loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    elmo_model.train()\n    classifier.train()\n    total_loss = 0\n\n    for input_data, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        input_data, labels = input_data.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        _, _, embeddings = elmo_model(input_data)\n\n        outputs = classifier(embeddings)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Average Loss Epoch {epoch+1}: {total_loss / len(train_loader)}\")\n\ndef extract_features(dataloader, elmo_model, classifier, device):\n    elmo_model.eval()\n    classifier.eval()\n    all_embeddings = []\n    all_labels = []\n\n    with torch.no_grad():\n        for input_data, labels in dataloader:\n            input_data = input_data.to(device)\n            _, _, embeddings = elmo_model(input_data)\n            outputs = classifier(embeddings)\n            all_embeddings.extend(outputs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return np.array(all_embeddings), np.array(all_labels)\n\n# Evaluate the model\ntest_embeddings, test_labels = extract_features(test_loader, elmo_model, classifier, device)\npredicted_labels = np.argmax(test_embeddings, axis=1)\n\nprint(classification_report(test_labels, predicted_labels))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T10:38:54.615860Z","iopub.execute_input":"2024-04-14T10:38:54.616282Z","iopub.status.idle":"2024-04-14T10:40:30.489105Z","shell.execute_reply.started":"2024-04-14T10:38:54.616250Z","shell.execute_reply":"2024-04-14T10:40:30.488000Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 303/303 [00:18<00:00, 16.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Loss Epoch 1: 0.4158588737101838\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 303/303 [00:18<00:00, 16.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Loss Epoch 2: 0.18889940986809362\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 303/303 [00:18<00:00, 16.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Loss Epoch 3: 0.14980837889323043\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 303/303 [00:18<00:00, 16.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Loss Epoch 4: 0.12693508030391812\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 303/303 [00:18<00:00, 16.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Loss Epoch 5: 0.11668757034201335\n              precision    recall  f1-score   support\n\n           0       0.92      0.89      0.90       335\n           1       0.98      0.79      0.88       116\n           2       0.94      0.98      0.96       759\n\n    accuracy                           0.94      1210\n   macro avg       0.95      0.89      0.91      1210\nweighted avg       0.94      0.94      0.94      1210\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}